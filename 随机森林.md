随机森林就是通过集成学习的Bagging思想将多棵树集成的一种算法：它的基本单元就是决策树。随机森林的名称中有两个关键词，一个是 **“随机”** ，一个就是 **“森林”**  。 **“森林”** 很好理解，一棵叫做树，那么成百上千棵就可以叫做森林了，其实这也是随机森林的主要思想--集成思想的体现。 **“随机”** 的含义我们会在下面讲到。  
我们要将一个输入样本进行分类，就需要将它输入到每棵树中进行分类。将若干个弱分类器的分类结果进行 **投票选择** ，从而组成一个强分类器，这就是随机森林 **bagging** 的思想：  
（1）如果训练集大小为N，对于每棵树而言， **随机** 且 **有放回** 地从训练集中的抽取N个训练样本（就是bootstrap sample方法, 拔靴法采样）作为该树的训练集；从这里我们可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本。  
（2）如果存在M个特征，则在每个节点分裂的时候，从M中 **随机** 选择m个特征维度（m << M），使用这些m个特征维度中最佳特征(最大化信息增益)来分割节点。在森林生长期间，m的值保持不变。  
一开始我们提到的随机森林中的 **“随机”** 就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。
